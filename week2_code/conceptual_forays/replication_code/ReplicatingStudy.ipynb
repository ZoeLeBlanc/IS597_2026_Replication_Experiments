{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating Study: Theory of Analysis\n",
    "\n",
    "This notebook replicates the R analysis pipeline in Python.\n",
    "\n",
    "## Steps:\n",
    "1. Preprocessing data (from 1preprocessingdata.R)\n",
    "2. Query Wikipedia (from 2_query_wikipedia.py)\n",
    "3. Calculate Jaccard distance for categories (from 3jac_distance_categories.R)\n",
    "4. Search Wikidata (from 4search_wikidata.R)\n",
    "5. Explore theorists (from 5explore_theorists.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set data directory\n",
    "DATA_DIR = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocessing Data\n",
    "\n",
    "Replication of `1preprocessingdata.R`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to clean column names (similar to janitor's clean_names)\n",
    "def clean_names(df):\n",
    "    \"\"\"Clean column names: lowercase, replace spaces/special chars with underscores\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.lower()\n",
    "        .str.replace(' ', '_', regex=False)\n",
    "        .str.replace('.', '_', regex=False)\n",
    "        .str.replace('*', '', regex=False)\n",
    "        .str.strip('_')\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ Theory shape: (549, 2)\n",
      "NN Theory shape: (434, 2)\n",
      "Theory shape: (3376, 4)\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "jjtheory = pd.read_csv(f\"{DATA_DIR}/1_JJ_theor_.csv\")\n",
    "nntheory = pd.read_csv(f\"{DATA_DIR}/1_NN_theor_.csv\")\n",
    "theory = pd.read_csv(f\"{DATA_DIR}/1_theor_of_normalized.csv\")\n",
    "\n",
    "print(f\"JJ Theory shape: {jjtheory.shape}\")\n",
    "print(f\"NN Theory shape: {nntheory.shape}\")\n",
    "print(f\"Theory shape: {theory.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ Theory columns: ['clustered_JJ_theor*', 'token_count']\n",
      "NN Theory columns: ['clustered_NN_theor*', 'token_count']\n",
      "Theory columns: ['.', 'query_string', 'normalized_string', 'freq']\n"
     ]
    }
   ],
   "source": [
    "# Check original column names\n",
    "print(\"JJ Theory columns:\", jjtheory.columns.tolist())\n",
    "print(\"NN Theory columns:\", nntheory.columns.tolist())\n",
    "print(\"Theory columns:\", theory.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning:\n",
      "JJ Theory columns: ['clustered_jj_theor', 'token_count']\n",
      "NN Theory columns: ['clustered_nn_theor', 'token_count']\n",
      "Theory columns: ['', 'query_string', 'normalized_string', 'freq']\n"
     ]
    }
   ],
   "source": [
    "# Clean column names\n",
    "jjtheory = clean_names(jjtheory)\n",
    "nntheory = clean_names(nntheory)\n",
    "theory = clean_names(theory)\n",
    "\n",
    "print(\"After cleaning:\")\n",
    "print(\"JJ Theory columns:\", jjtheory.columns.tolist())\n",
    "print(\"NN Theory columns:\", nntheory.columns.tolist())\n",
    "print(\"Theory columns:\", theory.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clustered_string</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>literary theory</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linguistic theory</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>critical theory</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>general theory</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>linguistic theories</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      clustered_string  freq\n",
       "0      literary theory   157\n",
       "1    linguistic theory   104\n",
       "2      critical theory    94\n",
       "3       general theory    41\n",
       "4  linguistic theories    38"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns for consistency\n",
    "jjtheory = jjtheory.rename(columns={'clustered_jj_theor': 'clustered_string', 'token_count': 'freq'})\n",
    "nntheory = nntheory.rename(columns={'clustered_nn_theor': 'clustered_string', 'token_count': 'freq'})\n",
    "\n",
    "# For theory dataframe, rename the index column and normalize\n",
    "theory = theory.rename(columns={'_': 'index'})\n",
    "\n",
    "jjtheory.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_string</th>\n",
       "      <th>freq_theory</th>\n",
       "      <th>clustered_string</th>\n",
       "      <th>freq_jj</th>\n",
       "      <th>clustered_string_nn</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AI theory</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Actor-Network Theory</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Advanced Theory</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aesthetic Theory</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Appraisal Theory</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  normalized_string  freq_theory clustered_string  freq_jj  \\\n",
       "0               NaN          NaN              NaN      NaN   \n",
       "1               NaN          NaN              NaN      NaN   \n",
       "2               NaN          NaN              NaN      NaN   \n",
       "3               NaN          NaN              NaN      NaN   \n",
       "4               NaN          NaN              NaN      NaN   \n",
       "\n",
       "    clustered_string_nn  freq  \n",
       "0             AI theory   3.0  \n",
       "1  Actor-Network Theory   7.0  \n",
       "2       Advanced Theory   1.0  \n",
       "3      Aesthetic Theory   3.0  \n",
       "4      Appraisal Theory   1.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join all \"theories of\" strings using full outer join\n",
    "# First merge theory with jjtheory\n",
    "theoriesof = pd.merge(\n",
    "    theory[['normalized_string', 'freq']], \n",
    "    jjtheory, \n",
    "    left_on='normalized_string', \n",
    "    right_on='clustered_string',\n",
    "    how='outer',\n",
    "    suffixes=('_theory', '_jj')\n",
    ")\n",
    "\n",
    "# Then merge with nntheory\n",
    "theoriesof = pd.merge(\n",
    "    theoriesof,\n",
    "    nntheory,\n",
    "    left_on='normalized_string',\n",
    "    right_on='clustered_string',\n",
    "    how='outer',\n",
    "    suffixes=('', '_nn')\n",
    ")\n",
    "\n",
    "theoriesof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_string</th>\n",
       "      <th>freq_theory</th>\n",
       "      <th>clustered_string</th>\n",
       "      <th>freq_jj</th>\n",
       "      <th>clustered_string_nn</th>\n",
       "      <th>freq</th>\n",
       "      <th>combined_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI theory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AI theory</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor-Network Theory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Actor-Network Theory</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advanced Theory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Advanced Theory</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aesthetic Theory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aesthetic Theory</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Appraisal Theory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Appraisal Theory</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      normalized_string  freq_theory clustered_string  freq_jj  \\\n",
       "0             AI theory          NaN              NaN      NaN   \n",
       "1  Actor-Network Theory          NaN              NaN      NaN   \n",
       "2       Advanced Theory          NaN              NaN      NaN   \n",
       "3      Aesthetic Theory          NaN              NaN      NaN   \n",
       "4      Appraisal Theory          NaN              NaN      NaN   \n",
       "\n",
       "    clustered_string_nn  freq combined_freq  \n",
       "0             AI theory   3.0           3.0  \n",
       "1  Actor-Network Theory   7.0           7.0  \n",
       "2       Advanced Theory   1.0           1.0  \n",
       "3      Aesthetic Theory   3.0           3.0  \n",
       "4      Appraisal Theory   1.0           1.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill normalized_string with values from clustered_string columns where null\n",
    "theoriesof['normalized_string'] = (\n",
    "    theoriesof['normalized_string']\n",
    "    .fillna(theoriesof['clustered_string'])\n",
    "    .fillna(theoriesof['clustered_string_nn'] if 'clustered_string_nn' in theoriesof.columns else '')\n",
    ")\n",
    "\n",
    "# Unite freq columns (combine all frequency values)\n",
    "freq_cols = [col for col in theoriesof.columns if 'freq' in col.lower()]\n",
    "theoriesof['combined_freq'] = theoriesof[freq_cols].fillna('').astype(str).agg('|'.join, axis=1)\n",
    "theoriesof['combined_freq'] = theoriesof['combined_freq'].str.replace(r'\\|+', '|', regex=True).str.strip('|')\n",
    "\n",
    "theoriesof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of normalized strings:\n",
      "normalized_string\n",
      "theory of language          16\n",
      "theory of meaning           13\n",
      "theory of literature        11\n",
      "theory of communication     10\n",
      "theory of .                  9\n",
      "theory of games              8\n",
      "theory of evolution          8\n",
      "theory of discourse          8\n",
      "theories of language         7\n",
      "theory of computation        7\n",
      "theory of verse              7\n",
      "theory of grammar            6\n",
      "theory of mind               6\n",
      "theory of metaphor           6\n",
      "theory of learning           6\n",
      "theory of everything         6\n",
      "theory of signs              6\n",
      "theory of style              6\n",
      "theory of lexicography       6\n",
      "theories of literature       6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count normalized strings\n",
    "print(\"Count of normalized strings:\")\n",
    "print(theoriesof['normalized_string'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique theories of: 3224\n",
      "Saved to ../data/1_theoriesof_complete_rerun.csv\n"
     ]
    }
   ],
   "source": [
    "# Filter unique \"theories of\" strings\n",
    "unique_theoriesof = theoriesof[['normalized_string']].drop_duplicates()\n",
    "print(f\"Unique theories of: {len(unique_theoriesof)}\")\n",
    "\n",
    "# Write csv with unique \"theor* of\" strings\n",
    "unique_theoriesof.to_csv(f\"{DATA_DIR}/1_theoriesof_complete_rerun.csv\", index=False)\n",
    "print(f\"Saved to {DATA_DIR}/1_theoriesof_complete_rerun.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to A (JJ): 513 items\n",
      "Unique to B (NN): 398 items\n",
      "Unique to C (theory): 2276 items\n"
     ]
    }
   ],
   "source": [
    "# Compare different \"theor* of\" sets\n",
    "# Read original data again to get clean sets\n",
    "jj_orig = pd.read_csv(f\"{DATA_DIR}/1_JJ_theor_.csv\")\n",
    "nn_orig = pd.read_csv(f\"{DATA_DIR}/1_NN_theor_.csv\")\n",
    "theory_orig = pd.read_csv(f\"{DATA_DIR}/1_theor_of_normalized.csv\")\n",
    "\n",
    "set_A = set(jj_orig.iloc[:, 0].dropna().str.strip())\n",
    "set_B = set(nn_orig.iloc[:, 0].dropna().str.strip())\n",
    "set_C = set(theory_orig['normalized_string'].dropna().str.strip())\n",
    "\n",
    "myl = {'A (JJ)': set_A, 'B (NN)': set_B, 'C (theory)': set_C}\n",
    "\n",
    "# Find differences - items unique to each set\n",
    "differences = {}\n",
    "for k, v in myl.items():\n",
    "    other_sets = [s for name, s in myl.items() if name != k]\n",
    "    differences[k] = v - set.union(*other_sets)\n",
    "\n",
    "for k, v in differences.items():\n",
    "    print(f\"Unique to {k}: {len(v)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Query Wikipedia\n",
    "\n",
    "Replication of `2_query_wikipedia.py`\n",
    "\n",
    "This step queries Wikipedia API to find categories matching our theory strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3223 theory strings\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theory of language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>theory of literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>theory of narrative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theory of meaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theory of .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      normalized_string\n",
       "0    theory of language\n",
       "1  theory of literature\n",
       "2   theory of narrative\n",
       "3     theory of meaning\n",
       "4           theory of ."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the complete theories file\n",
    "# Use the existing file if available, otherwise use the one we just created\n",
    "if os.path.exists(f\"{DATA_DIR}/1_theoriesof_complete.csv\"):\n",
    "    string_df = pd.read_csv(f\"{DATA_DIR}/1_theoriesof_complete.csv\")\n",
    "else:\n",
    "    string_df = pd.read_csv(f\"{DATA_DIR}/1_theoriesof_complete_rerun.csv\")\n",
    "\n",
    "print(f\"Loaded {len(string_df)} theory strings\")\n",
    "string_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_wikipedia_categories(query_string):\n",
    "    \"\"\"Query Wikipedia API for categories matching the query string\"\"\"\n",
    "    url = f\"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'list': 'search',\n",
    "        'utf8': 1,\n",
    "        'srsearch': query_string,\n",
    "        'srnamespace': 14  # Category namespace\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        search_results = data.get('query', {}).get('search', [])\n",
    "        return search_results\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying '{query_string}': {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we already have the Wikipedia categories data\n",
    "wiki_categories_file = f\"{DATA_DIR}/2_wikipediacategoriesfromquery.csv\"\n",
    "\n",
    "if os.path.exists(wiki_categories_file):\n",
    "    print(f\"Loading existing Wikipedia categories from {wiki_categories_file}\")\n",
    "    all_results_df = pd.read_csv(wiki_categories_file)\n",
    "else:\n",
    "    print(\"Querying Wikipedia API (this may take a while)...\")\n",
    "    \n",
    "    # Get all query strings\n",
    "    query_strings = string_df['normalized_string'].dropna().tolist()\n",
    "    \n",
    "    # Query Wikipedia and collect results\n",
    "    all_results = []\n",
    "    for i, query_string in enumerate(query_strings):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing {i}/{len(query_strings)}...\")\n",
    "        \n",
    "        results = query_wikipedia_categories(query_string)\n",
    "        for result in results:\n",
    "            result['query_string'] = query_string\n",
    "            all_results.append(result)\n",
    "    \n",
    "    all_results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Create category column without \"Category:\" prefix\n",
    "    if 'title' in all_results_df.columns:\n",
    "        all_results_df['category'] = all_results_df['title'].str.replace('Category:', '', regex=False)\n",
    "    \n",
    "    # Save results\n",
    "    all_results_df.to_csv(wiki_categories_file, index=False)\n",
    "    print(f\"Saved {len(all_results_df)} results to {wiki_categories_file}\")\n",
    "\n",
    "print(f\"Wikipedia categories shape: {all_results_df.shape}\")\n",
    "all_results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Jaccard Distance for Categories\n",
    "\n",
    "Replication of `3jac_distance_categories.R`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install textdistance if needed (for Jaccard and Levenshtein distances)\n",
    "try:\n",
    "    import textdistance\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'textdistance'])\n",
    "    import textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read categories retrieved from matching with \"theory of\" strings\n",
    "categories_wikipedia = pd.read_csv(f\"{DATA_DIR}/2_wikipediacategoriesfromquery.csv\")\n",
    "\n",
    "print(f\"Columns: {categories_wikipedia.columns.tolist()}\")\n",
    "print(f\"Unique titles: {categories_wikipedia['title'].nunique()}\")\n",
    "print(f\"Unique query strings: {categories_wikipedia['query_string'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of categories by each \"theory of\" string\n",
    "catcountbystring = categories_wikipedia.groupby('query_string')['title'].nunique().reset_index()\n",
    "catcountbystring.columns = ['query_string', 'count']\n",
    "\n",
    "print(\"Categories count by query string:\")\n",
    "print(catcountbystring.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of number of categories by string query\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(catcountbystring['count'], bins=30, color='#112446', edgecolor='white')\n",
    "    plt.title('Histogram of dif. categories retrieved by query string')\n",
    "    plt.xlabel('Number of categories')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"matplotlib not available, skipping visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete \"Category:\" string to match and compare with query string\n",
    "categories_wikipedia['category'] = categories_wikipedia['title'].str.replace('Category:', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_qgram(s1, s2, q=3):\n",
    "    \"\"\"Calculate Jaccard distance using q-grams (similar to R stringdist with method='jaccard')\"\"\"\n",
    "    if pd.isna(s1) or pd.isna(s2):\n",
    "        return np.nan\n",
    "    s1, s2 = str(s1).lower(), str(s2).lower()\n",
    "    return 1 - textdistance.jaccard.normalized_similarity(s1, s2)\n",
    "\n",
    "def calculate_levenshtein(s1, s2):\n",
    "    \"\"\"Calculate Levenshtein distance\"\"\"\n",
    "    if pd.isna(s1) or pd.isna(s2):\n",
    "        return np.nan\n",
    "    return textdistance.levenshtein.distance(str(s1), str(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Jaccard and Levenshtein distances\n",
    "print(\"Calculating distances (this may take a moment)...\")\n",
    "\n",
    "categories_wikipedia['jac'] = categories_wikipedia.apply(\n",
    "    lambda row: calculate_jaccard_qgram(row['category'], row['query_string']), axis=1\n",
    ")\n",
    "\n",
    "categories_wikipedia['lev'] = categories_wikipedia.apply(\n",
    "    lambda row: calculate_levenshtein(row['category'], row['query_string']), axis=1\n",
    ")\n",
    "\n",
    "print(\"Distance calculations complete\")\n",
    "categories_wikipedia[['category', 'query_string', 'jac', 'lev']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category count for wordcloud\n",
    "cat_count = categories_wikipedia.groupby('category').size().reset_index(name='n')\n",
    "print(f\"Unique categories: {len(cat_count)}\")\n",
    "cat_count.nlargest(20, 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate wordcloud (optional)\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create word frequencies dict\n",
    "    word_freq = dict(zip(cat_count['category'], cat_count['n']))\n",
    "    \n",
    "    # Filter to categories with min frequency\n",
    "    word_freq = {k: v for k, v in word_freq.items() if v >= 5}\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=800, height=400,\n",
    "        background_color='white',\n",
    "        max_words=200\n",
    "    ).generate_from_frequencies(word_freq)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Categories Wordcloud')\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"wordcloud library not available, skipping visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter categories by Jaccard distance\n",
    "df_filtered = categories_wikipedia[categories_wikipedia['jac'] < 0.6].copy()\n",
    "print(f\"After jac < 0.6 filter: {len(df_filtered)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by strange keywords found in wordcloud\n",
    "exclude_pattern = r'WikiProject|Wikipedia|[C|c]onspiracy|Christ|[M|m]ilitary|articles|journals|missing|Satanic|[T|t]errorism|abuse|backlog|Lists|albums'\n",
    "snippet_exclude = r'[C|c]onspiracy|[T|t]elevision|Nazis'\n",
    "\n",
    "df_filtered = categories_wikipedia[\n",
    "    (categories_wikipedia['jac'] < 0.6) &\n",
    "    (~categories_wikipedia['category'].str.contains(exclude_pattern, regex=True, na=False)) &\n",
    "    (~categories_wikipedia['snippet'].str.contains(snippet_exclude, regex=True, na=False))\n",
    "].copy()\n",
    "\n",
    "print(f\"After filtering: {len(df_filtered)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered categories\n",
    "df_filtered.to_csv(f\"{DATA_DIR}/3_wikicategories_distances_filtered_rerun.csv\", index=False)\n",
    "print(f\"Saved filtered categories to {DATA_DIR}/3_wikicategories_distances_filtered_rerun.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Search Wikidata\n",
    "\n",
    "Replication of `4search_wikidata.R`\n",
    "\n",
    "This step searches Wikidata for humans in Wikipedia categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_wikidata(query):\n",
    "    \"\"\"Query Wikidata SPARQL endpoint\"\"\"\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params={'query': query}, headers=headers, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        results = data.get('results', {}).get('bindings', [])\n",
    "        return [{\n",
    "            'item': r.get('item', {}).get('value', ''),\n",
    "            'itemLabel': r.get('itemLabel', {}).get('value', '')\n",
    "        } for r in results]\n",
    "    except Exception as e:\n",
    "        print(f\"Query error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_query(category):\n",
    "    \"\"\"Create SPARQL query to search humans inside a Wikipedia category\"\"\"\n",
    "    return f'''SELECT ?item ?itemLabel WHERE {{\n",
    "  BIND(\"{category}\" as ?category)\n",
    "  SERVICE wikibase:mwapi {{\n",
    "     bd:serviceParam wikibase:endpoint \"en.wikipedia.org\";\n",
    "                     wikibase:api \"Generator\";\n",
    "                     mwapi:generator \"categorymembers\";\n",
    "                     mwapi:gcmtitle ?category.\n",
    "     ?item wikibase:apiOutputItem mwapi:item.\n",
    "  }} \n",
    "  FILTER BOUND (?item)\n",
    "  FILTER EXISTS {{\n",
    "    ?article schema:about ?item .\n",
    "    ?item wdt:P31 wd:Q5.\n",
    "  }}\n",
    "SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}    \n",
    "}}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filtered data\n",
    "if os.path.exists(f\"{DATA_DIR}/3_wikicategories_distances_filtered.csv\"):\n",
    "    data = pd.read_csv(f\"{DATA_DIR}/3_wikicategories_distances_filtered.csv\")\n",
    "else:\n",
    "    data = pd.read_csv(f\"{DATA_DIR}/3_wikicategories_distances_filtered_rerun.csv\")\n",
    "\n",
    "print(f\"Loaded {len(data)} filtered categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we already have the Wikidata humans data\n",
    "humans_file = f\"{DATA_DIR}/4_theorystrings_categories_humans.csv\"\n",
    "\n",
    "if os.path.exists(humans_file):\n",
    "    print(f\"Loading existing Wikidata humans from {humans_file}\")\n",
    "    df_humans = pd.read_csv(humans_file)\n",
    "else:\n",
    "    print(\"Querying Wikidata (this may take a while)...\")\n",
    "    \n",
    "    # Get unique category titles\n",
    "    queries_titles = data['title'].unique().tolist()\n",
    "    print(f\"Querying {len(queries_titles)} unique categories\")\n",
    "    \n",
    "    all_results = []\n",
    "    for i, title in enumerate(queries_titles):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing {i}/{len(queries_titles)}...\")\n",
    "        \n",
    "        query = cat_query(title)\n",
    "        results = query_wikidata(query)\n",
    "        \n",
    "        for r in results:\n",
    "            r['category'] = title\n",
    "            all_results.append(r)\n",
    "    \n",
    "    df_humans = pd.DataFrame(all_results)\n",
    "    df_humans.to_csv(humans_file, index=False)\n",
    "    print(f\"Saved {len(df_humans)} results to {humans_file}\")\n",
    "\n",
    "print(f\"Humans data shape: {df_humans.shape}\")\n",
    "df_humans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique humans\n",
    "df_unique = df_humans.drop_duplicates(subset=['itemLabel']).copy()\n",
    "if 'category' in df_unique.columns:\n",
    "    df_unique = df_unique.drop(columns=['category'])\n",
    "\n",
    "print(f\"Unique humans: {len(df_unique)}\")\n",
    "\n",
    "# Save unique humans\n",
    "df_unique.to_csv(f\"{DATA_DIR}/4_theorystrings_categories_humans_unique.csv\", index=False)\n",
    "print(f\"Saved unique humans to {DATA_DIR}/4_theorystrings_categories_humans_unique.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Explore Theorists\n",
    "\n",
    "Replication of `5explore_theorists.R`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if extended data exists\n",
    "extended_file = f\"{DATA_DIR}/4_theory_dictionary_wikidata_extended.csv\"\n",
    "\n",
    "if os.path.exists(extended_file):\n",
    "    data = pd.read_csv(extended_file)\n",
    "    data.columns = data.columns.str.lower().str.replace(' ', '_')\n",
    "    \n",
    "    print(f\"Loaded extended data with {len(data)} rows\")\n",
    "    print(f\"Columns: {data.columns.tolist()}\")\n",
    "else:\n",
    "    # Use the humans data we have\n",
    "    data = pd.read_csv(f\"{DATA_DIR}/4_theorystrings_categories_humans.csv\")\n",
    "    data.columns = data.columns.str.lower().str.replace(' ', '_')\n",
    "    print(f\"Using humans data with {len(data)} rows\")\n",
    "    print(f\"Columns: {data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore distinct items - eliminate duplicates\n",
    "if 'wikidata_id' in data.columns:\n",
    "    unique_data = data.drop_duplicates(subset=['wikidata_id'])\n",
    "    id_col = 'wikidata_id'\n",
    "elif 'item' in data.columns:\n",
    "    unique_data = data.drop_duplicates(subset=['item'])\n",
    "    id_col = 'item'\n",
    "else:\n",
    "    unique_data = data.drop_duplicates(subset=['itemlabel'])\n",
    "    id_col = 'itemlabel'\n",
    "\n",
    "print(f\"Unique persons: {len(unique_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore genders if available\n",
    "if 'sex_or_gender' in unique_data.columns:\n",
    "    print(\"\\nGender distribution:\")\n",
    "    print(unique_data['sex_or_gender'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore countries if available\n",
    "if 'country_of_citizenship' in unique_data.columns:\n",
    "    print(\"\\nTop countries of citizenship:\")\n",
    "    print(unique_data['country_of_citizenship'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Total records: {len(data)}\")\n",
    "print(f\"Unique persons: {len(unique_data)}\")\n",
    "\n",
    "if 'category' in data.columns:\n",
    "    print(f\"Unique categories: {data['category'].nunique()}\")\n",
    "    \n",
    "    # Top categories by number of persons\n",
    "    print(\"\\nTop categories by person count:\")\n",
    "    print(data.groupby('category').size().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has replicated the following R scripts in Python:\n",
    "\n",
    "1. **1preprocessingdata.R** - Merged and cleaned theory strings from multiple sources\n",
    "2. **2_query_wikipedia.py** - Queried Wikipedia API for categories (already Python, updated)\n",
    "3. **3jac_distance_categories.R** - Calculated Jaccard distances and filtered categories\n",
    "4. **4search_wikidata.R** - Searched Wikidata for humans in categories\n",
    "5. **5explore_theorists.R** - Explored theorist demographics\n",
    "\n",
    "All output files are saved in the `../data` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
